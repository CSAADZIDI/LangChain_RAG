{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "Les systèmes **RAG (Retrieval-Augmented Generation)** dans LangChain permettent aux modèles de langage de s’appuyer sur des **connaissances externes** pour produire des réponses plus précises, actualisées et pertinentes.\n",
    "\n",
    "Contrairement à un simple LLM qui génère une réponse uniquement à partir de ce qu’il a appris pendant son entraînement, un système RAG interroge une base de documents pour retrouver des morceaux d’information pertinents – appelés **chunks** – et les injecte dans le prompt du LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb160db",
   "metadata": {},
   "source": [
    "![RAG](img/rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3075ae",
   "metadata": {},
   "source": [
    "**Que montre le schéma ci-dessus ?**\n",
    "\n",
    "Le processus se divise en **deux grandes phases** : **préparation des documents** et **traitement des requêtes**.\n",
    "\n",
    "**Préparation des documents (à gauche)**\n",
    "- (1) Un fichier (document source) est divisé en **chunks**, c’est-à-dire en petits segments de texte.\n",
    "- (2) Chaque chunk est passé dans un LLM Embedder, un encodeur qui transforme le texte en un vecteur numérique (**embeddings**).\n",
    "- (3) Ces vecteurs sont ensuite stockés dans un Vector Store, une base de données spécialisée pour les recherches par **similarité sémantique**.\n",
    "\n",
    "**Traitement des requêtes (à droite)**\n",
    "- (a) Lorsqu’un utilisateur emet une requête, celle-ci est à son tour encodée via **le même LLM Embedder** pour obtenir son vecteur.\n",
    "- (b) Ce vecteur est utilisé par le **Retriever**, qui compare la requête aux vecteurs des **chunks** pour trouver les plus similaires.\n",
    "- (c) Les chunks retrouvés sont envoyés au LLM, qui les utilise comme contexte pour formuler une réponse.\n",
    "\n",
    "\n",
    "En résumé, ce fonctionnement est illustré par la boucle :\n",
    "\n",
    "> Requête → Encodage → Recherche dans la base vectorielle → Récupération des chunks → Passage au LLM → Réponse contextuelle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du modèle LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un modèle de langage local grâce à **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion à une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d’interagir facilement avec un modèle comme **llama3** ainsi qu'un **modèle d'embeddings** déjà téléchargés via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7301c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Chargement des clés d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des modèles en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3\", temperature=0)\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))\n",
    "\n",
    "# Modèle spécialisé pour convertir du texte en vecteurs (https://ollama.com/library/nomic-embed-text).\n",
    "# Il existe d'autres modèles d'embeddings (comme \"all-MiniLM-L6-v2\", \"text-embedding-ada-002\", etc.) \n",
    "# avec des performances et dimensions variées selon les cas d’usage (recherche sémantique, classification, etc.).\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. RAG standard\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3369916",
   "metadata": {},
   "source": [
    "Le **RAG standard** consiste à :\n",
    "- formuler une requête explicite\n",
    "- interroger une base de documents vectorisée\n",
    "- utiliser un modèle LLM pour générer une réponse à partir des résultats retrouvés. \n",
    " \n",
    "Ce pipeline est **efficace pour des questions indépendantes, sans contexte conversationnel**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7de366",
   "metadata": {},
   "source": [
    "### 2.1 Préparation des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699bd52",
   "metadata": {},
   "source": [
    "Nous initialisons les chemins nécessaires à la préparation des documents d’entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77475b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupère le chemin absolu du répertoire courant (là où le script est exécuté)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Nom du fichier texte contenant les comptes rendus de réunion\n",
    "file_name = \"meeting_reports.txt\"\n",
    "\n",
    "# Construit le chemin complet vers le fichier texte dans le dossier \"data\"\n",
    "file_path = os.path.join(current_dir, \"data\", file_name)\n",
    "\n",
    "# Définit le chemin du répertoire où sera stockée la base de données vectorielle (Chroma DB)\n",
    "db_dir = os.path.join(current_dir, \"data\", \"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed30a56",
   "metadata": {},
   "source": [
    "### 2.2 Initialisation du vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c526e6f",
   "metadata": {},
   "source": [
    "Nous vérifions ici si la base vectorielle existe déjà.  \n",
    "Si ce n’est pas le cas, le fichier source est chargé, découpé en morceaux, enrichi de métadonnées, puis indexé dans Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9884d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(db_dir):\n",
    "    print(\"Initializing vector store...\")\n",
    "\n",
    "    # Chargement du fichier texte brut contenant les documents\n",
    "    loader = TextLoader(file_path)\n",
    "    loaded_document = loader.load()\n",
    "\n",
    "    # Découpage du document en chunks de 1000 caractères avec un chevauchement de 0\n",
    "    # - chunk_size détermine la taille maximale de chaque morceau (en nombre de caractères ici : 1000)\n",
    "    # - chunk_overlap permet de conserver un chevauchement entre les morceaux pour éviter les coupures abruptes, ici il est à 0, donc sans recouvrement.\n",
    "    # - RecursiveCharacterTextSplitter est souvent préféré en pratique pour des documents textuels comme des comptes rendus, \n",
    "    #   des articles ou de la documentation technique, car il garde mieux le contexte sémantique.\n",
    "    #   Ce splitter tente d'abord de découper sur les sauts de ligne, puis sur les phrases, puis sur les mots, etc.\n",
    "    # ... d'autres Text Splitter comme CharachterTextSplitter existent. À approfondir si besoin\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "    # Ajout de métadonnées à chaque chunk (utile pour le filtrage ou le suivi de provenance).\n",
    "    # Ici 2 metadata sont ajoutés mais il pourrait en y avoir plus.\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"source\"] = file_path    # Chemin d'origine du document\n",
    "        chunk.metadata[\"category\"] = \"meeting\"  # Catégorie de contenu (à adapter selon les besoins)\n",
    "\n",
    "    # Création et persistance de la base vectorielle dans le dossier défini\n",
    "    db = Chroma.from_documents(chunks, embedder, persist_directory=db_dir)\n",
    "\n",
    "    print(\"Vector store created !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbb986",
   "metadata": {},
   "source": [
    "### 2.3 Initialisation du moteur de recherche vectorielle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc3f2",
   "metadata": {},
   "source": [
    "Une fois la base vectorielle Chroma initialisée avec les embeddings, nous la transformons en **moteur de recherche (retriever)**.  \n",
    "Cela permet de retrouver les documents les plus proches sémantiquement d’une question ou d’une requête.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee3ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8996\\573588974.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n"
     ]
    }
   ],
   "source": [
    "# Chargement de la base vectorielle existante, avec liaison avec le même embedder ayant servi pour créer la base vectorielle\n",
    "db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n",
    "\n",
    "# Conversion de la base Chroma en \"retriever\" pour effectuer des recherches par similarité\n",
    "# - search_type=\"similarity\" utilise la distance cosinus entre les vecteurs\n",
    "# - \"k\": 3 signifie que l'on souhaite récupérer les 3 documents les plus proches\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# 💡 Il est aussi possible d’utiliser d’autres types de recherche (search_type) :\n",
    "# - \"mmr\" (Maximal Marginal Relevance) : équilibre entre pertinence et diversité des résultats\n",
    "# - \"similarity_score_threshold\" : retourne uniquement les documents dont le score dépasse un certain seuil\n",
    "#      search_kwargs={\"score_threshold\": 0.8} permet par exemple de filtrer les résultats peu pertinents\n",
    "#\n",
    "# D’autres paramètres utiles dans search_kwargs :\n",
    "# - \"fetch_k\" : nombre de documents à récupérer avant le tri final (utile avec MMR)\n",
    "# - \"lambda_mult\" : pondération entre pertinence et diversité dans MMR\n",
    "# \n",
    "# Etc... à approfondir si besoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3318104",
   "metadata": {},
   "source": [
    "### 2.4 Exécution d’une requête de recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3186eb",
   "metadata": {},
   "source": [
    "Dans cette étape, nous combinons la recherche vectorielle avec un LLM.  \n",
    "L’objectif est de fournir une réponse pertinente à une question, en s’appuyant uniquement sur les documents retrouvés dans la base vectorielle.  \n",
    "Le modèle est guidé par un prompt structuré qui inclut la requête initiale et les contenus des chunks pertinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f69c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "En examinant les documents, j'ai trouvé des informations relatives aux réunions de la société Neolink.\n",
       "\n",
       "Selon le document intitulé \"Calendrier des réunions 2022\", il y a eu plusieurs réunions concernant la société Neolink au cours de l'année 2022. Voici quelques-unes de ces réunions :\n",
       "\n",
       "* Réunion du conseil d'administration : 15 mars 2022, 10h00\n",
       "* Séance de formation pour les nouveaux employés : 22 mars 2022, 14h00\n",
       "* Réunion des équipes de vente : 5 avril 2022, 9h30\n",
       "* Assemblée générale annuelle : 12 mai 2022, 10h30\n",
       "* Réunion du comité de direction : 26 juillet 2022, 11h00\n",
       "* Séance de rétroaction des employés : 15 septembre 2022, 14h30\n",
       "\n",
       "Il est possible que d'autres réunions aient eu lieu ou soient prévues, mais ces informations sont basées sur les documents fournis.\n",
       "\n",
       "Si vous avez d'autres questions ou si vous souhaitez des informations supplémentaires, n'hésitez pas à me demander !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Requête posée par l'utilisateur\n",
    "query = \"Quels sont les réunions concernant la société Neolink ?\"\n",
    "\n",
    "# Recherche des chunks vectoriellement proches de la question\n",
    "relevant_chunks = retriever.invoke(query)\n",
    "\n",
    "# Optionnel : affichage manuel des chunks retrouvés (utile pour debug ou vérification)\n",
    "# for i, chunk in enumerate(relevant_chunks, 1):\n",
    "#     print(f\"Chunk {i}:\\n{chunk.page_content}\\n\")\n",
    "\n",
    "# Construction du message d'entrée à envoyer au modèle\n",
    "# Nous incluons la question et le contenu des documents pour contraindre le LLM à ne répondre qu'en s'appuyant sur ces sources\n",
    "input_message = (\n",
    "    \"Voici des documents qui vont t'aider à répondre à la question : \"\n",
    "    + query\n",
    "    + \"\\n\\nDocuments pertinents : \\n\"\n",
    "    + \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "    + \"\\n\\nDonne une réponse basée uniquement sur les documents qui te sont fournis.\"\n",
    ")\n",
    "\n",
    "# Construction du message complet pour le LLM, avec un rôle système et un message utilisateur\n",
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un assistant qui aide à retrouver tout type d'informations interne à une entreprise\"),\n",
    "    HumanMessage(content=input_message)\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c362eac",
   "metadata": {},
   "source": [
    "### 🧩 Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ccbc1",
   "metadata": {},
   "source": [
    "La société NovTech gère de nombreux documents internes :\n",
    "- des rapports d’incidents (panne, erreur technique, post-mortem),\n",
    "- des procédures opérationnelles (onboarding, accès système, déploiement…).\n",
    "\n",
    "Actuellement, les équipes perdent du temps à chercher les bonnes informations à travers des fichiers éparpillés.\n",
    "\n",
    "Votre objectif est de construire un assistant basé sur l'architecture RAG qui permettra :\n",
    "- de retrouver rapidement les procédures en cas de besoin,\n",
    "- de consulter les résolutions d’incidents similaires,\n",
    "- de répondre à des questions en langage naturel en s’appuyant uniquement sur les documents internes.\n",
    "\n",
    "Pour vous aider, vous pouvez suivre les étapes suivantes :\n",
    "1. Chargement des documents\n",
    "2. Découpage en chunks\n",
    "3. Indexation vectorielle\n",
    "4. Recherche contextuelle\n",
    "5. Génération de réponse\n",
    "\n",
    "ℹ️ Les documents de l'entreprise se trouve dans le dossier `data/novtech`.  \n",
    "💪🏻 **Bonus** : Rendre possible un filtrage par catégorie dans les recherches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25744402",
   "metadata": {},
   "source": [
    "#### 1. chargement des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffe3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def load_documents_from_folders(folders):\n",
    "    all_chunks = []\n",
    "    all_docs = []\n",
    "    print(\"forlders\",folders)\n",
    "    print(os.listdir(folders))\n",
    "    for folder in os.listdir(folders):\n",
    "        path_folder = os.path.join(folders,folder)\n",
    "        for file in os.listdir(path_folder):\n",
    "            print(file)\n",
    "            file_path = os.path.join(path_folder, file)\n",
    "            print(file_path)\n",
    "            if os.path.isfile(file_path):\n",
    "                loader = TextLoader(file_path)\n",
    "                loaded_document = loader.load()\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "                chunks = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "    # Ajout de métadonnées à chaque chunk (utile pour le filtrage ou le suivi de provenance).\n",
    "    # Ici 2 metadata sont ajoutés mais il pourrait en y avoir plus.\n",
    "                for chunk in chunks:\n",
    "                    chunk.metadata[\"source\"] = file_path    # Chemin d'origine du document\n",
    "                    chunk.metadata[\"category\"] = folder.split(\"/\")[-1]  # Catégorie de contenu (à adapter selon les besoins)\n",
    "\n",
    "                all_chunks.extend(chunks)\n",
    "                all_docs.extend(loaded_document)\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103785c8",
   "metadata": {},
   "source": [
    "#### 2. Initialisation du db vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d078bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "    \n",
    "folders_path = os.path.join(current_dir,\"data/novtech/\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8bca326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forlders c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/\n",
      "['incidents', 'procedures']\n",
      "incident_01.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_01.txt\n",
      "incident_02.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_02.txt\n",
      "incident_03.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_03.txt\n",
      "incident_04.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_04.txt\n",
      "incident_05.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_05.txt\n",
      "incident_06.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_06.txt\n",
      "incident_07.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_07.txt\n",
      "incident_08.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_08.txt\n",
      "incident_09.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_09.txt\n",
      "incident_10.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_10.txt\n",
      "incident_11.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_11.txt\n",
      "incident_12.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_12.txt\n",
      "incident_13.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_13.txt\n",
      "incident_14.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_14.txt\n",
      "incident_15.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_15.txt\n",
      "incident_16.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_16.txt\n",
      "incident_17.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_17.txt\n",
      "incident_18.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_18.txt\n",
      "incident_19.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_19.txt\n",
      "incident_20.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_20.txt\n",
      "incident_21.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_21.txt\n",
      "incident_22.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_22.txt\n",
      "incident_23.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_23.txt\n",
      "incident_24.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_24.txt\n",
      "incident_25.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_25.txt\n",
      "incident_26.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_26.txt\n",
      "incident_27.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_27.txt\n",
      "incident_28.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_28.txt\n",
      "incident_29.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_29.txt\n",
      "incident_30.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_30.txt\n",
      "incident_31.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_31.txt\n",
      "incident_32.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_32.txt\n",
      "incident_33.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_33.txt\n",
      "incident_34.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_34.txt\n",
      "incident_35.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_35.txt\n",
      "incident_36.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_36.txt\n",
      "incident_37.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_37.txt\n",
      "incident_38.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_38.txt\n",
      "incident_39.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_39.txt\n",
      "incident_40.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_40.txt\n",
      "incident_41.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_41.txt\n",
      "incident_42.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_42.txt\n",
      "incident_43.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_43.txt\n",
      "incident_44.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_44.txt\n",
      "incident_45.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_45.txt\n",
      "incident_46.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_46.txt\n",
      "incident_47.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_47.txt\n",
      "incident_48.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_48.txt\n",
      "incident_49.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_49.txt\n",
      "incident_50.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_50.txt\n",
      "procedure_01.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_01.txt\n",
      "procedure_02.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_02.txt\n",
      "procedure_03.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_03.txt\n",
      "procedure_04.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_04.txt\n",
      "procedure_05.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_05.txt\n",
      "procedure_06.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_06.txt\n",
      "procedure_07.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_07.txt\n",
      "procedure_08.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_08.txt\n",
      "procedure_09.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_09.txt\n",
      "procedure_10.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_10.txt\n",
      "procedure_11.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_11.txt\n",
      "procedure_12.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_12.txt\n",
      "procedure_13.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_13.txt\n",
      "procedure_14.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_14.txt\n",
      "procedure_15.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_15.txt\n",
      "procedure_16.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_16.txt\n",
      "procedure_17.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_17.txt\n",
      "procedure_18.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_18.txt\n",
      "procedure_19.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_19.txt\n",
      "procedure_20.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_20.txt\n",
      "procedure_21.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_21.txt\n",
      "procedure_22.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_22.txt\n",
      "procedure_23.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_23.txt\n",
      "procedure_24.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_24.txt\n",
      "procedure_25.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_25.txt\n",
      "procedure_26.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_26.txt\n",
      "procedure_27.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_27.txt\n",
      "procedure_28.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_28.txt\n",
      "procedure_29.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_29.txt\n",
      "procedure_30.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_30.txt\n",
      "procedure_31.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_31.txt\n",
      "procedure_32.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_32.txt\n",
      "procedure_33.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_33.txt\n",
      "procedure_34.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_34.txt\n",
      "procedure_35.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_35.txt\n",
      "procedure_36.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_36.txt\n",
      "procedure_37.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_37.txt\n",
      "procedure_38.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_38.txt\n",
      "procedure_39.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_39.txt\n",
      "procedure_40.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_40.txt\n",
      "procedure_41.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_41.txt\n",
      "procedure_42.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_42.txt\n",
      "procedure_43.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_43.txt\n",
      "procedure_44.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_44.txt\n",
      "procedure_45.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_45.txt\n",
      "procedure_46.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_46.txt\n",
      "procedure_47.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_47.txt\n",
      "procedure_48.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_48.txt\n",
      "procedure_49.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_49.txt\n",
      "procedure_50.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_50.txt\n"
     ]
    }
   ],
   "source": [
    "all_loaded_docs = load_documents_from_folders(folders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512928f",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " # Définit le chemin du répertoire où sera stockée la base de données vectorielle (Chroma DB)\n",
    "db_dir_novtech = os.path.join(folders_path, \"db_novtech\")\n",
    "# Création et persistance de la base vectorielle dans le dossier défini\n",
    "db = Chroma.from_documents(all_loaded_docs, embedder, persist_directory=db_dir_novtech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "998f0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5536\\2472262417.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=db_dir_novtech, embedding_function=embedder)\n"
     ]
    }
   ],
   "source": [
    "# Chargement de la base vectorielle existante, avec liaison avec le même embedder ayant servi pour créer la base vectorielle\n",
    "db = Chroma(persist_directory=db_dir_novtech, embedding_function=embedder)\n",
    "\n",
    "# Conversion de la base Chroma en \"retriever\" pour effectuer des recherches par similarité\n",
    "# - search_type=\"similarity\" utilise la distance cosinus entre les vecteurs\n",
    "# - \"k\": 3 signifie que l'on souhaite récupérer les 3 documents les plus proches\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3f439",
   "metadata": {},
   "source": [
    "#### 3 Exécution d’une requête de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49ae5cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Selon les rapports d'incident, il y a eu trois incidents concernant le service billing :\n",
       "\n",
       "* Incident #35 : erreur détectée sur service billing, durée 29 minutes, action corrective : redémarrage du service et ajout de monitoring spécifique.\n",
       "* Incident #30 : erreur détectée sur service billing, durée 39 minutes, action corrective : redémarrage du service et ajout de monitoring spécifique.\n",
       "* Incident #37 : erreur détectée sur service billing, durée 36 minutes, action corrective : redémarrage du service et ajout de monitoring spécifique.\n",
       "\n",
       "En résumé, il y a eu trois incidents concernant le service billing entre le 31 janvier 2024 et le 7 février 2024."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Votre code ici\n",
    "\n",
    "# Requête posée par l'utilisateur\n",
    "query = \"Quels sont les incidents concernant le service billing ?\"\n",
    "\n",
    "# Recherche des chunks vectoriellement proches de la question\n",
    "relevant_chunks = retriever.invoke(query)\n",
    "\n",
    "# Optionnel : affichage manuel des chunks retrouvés (utile pour debug ou vérification)\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "     print(f\"Chunk {i}:\\n{chunk.page_content}\\n\")\n",
    "\n",
    "# Construction du message d'entrée à envoyer au modèle\n",
    "# Nous incluons la question et le contenu des documents pour contraindre le LLM à ne répondre qu'en s'appuyant sur ces sources\n",
    "input_message = (\n",
    "    \"Voici des documents qui vont t'aider à répondre à la question : \"\n",
    "    + query\n",
    "    + \"\\n\\nDocuments pertinents : \\n\"\n",
    "    + \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "    + \"\\n\\nDonne une réponse basée uniquement sur les documents qui te sont fournis.\"\n",
    ")\n",
    "\n",
    "# Construction du message complet pour le LLM, avec un rôle système et un message utilisateur\n",
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un assistant qui aide à retrouver tout type d'informations interne à une entreprise\"),\n",
    "    HumanMessage(content=input_message)\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a000884",
   "metadata": {},
   "source": [
    "# 3. RAG conversationnel\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843e3e1",
   "metadata": {},
   "source": [
    "Dans un cadre d’**interaction continue**, les utilisateurs posent souvent des questions implicites ou référentielles (ex. “Et lui ?”). Le **RAG conversationnel** ajoute une étape clé : la **reformulation de la question en prenant en compte l’historique du dialogue**.  \n",
    "\n",
    "Cette version de RAG permet de maintenir la pertinence des recherches dans la base vectorielle tout en conservant la fluidité de la conversation, ce qui la rend adaptée aux assistants IA ou aux chatbots avancés.\n",
    "\n",
    "**Exemple**\n",
    "\n",
    "Historique de la conversation :\n",
    "- Utilisateur : *Qui est le CEO de Tesla ?*\n",
    "- IA : *Elon Musk est le CEO de Tesla*.\n",
    "- Utilisateur : *Et de SpaceX ?*\n",
    "\n",
    "➡️ La question “Et de SpaceX ?” est ambiguë seule. Le moteur de recherche (retriever) ne sait pas de quoi il s’agit exactement.\n",
    "\n",
    "Avec une reformulation de la question de l'utilisateur cela donnerait : “Qui est le CEO de SpaceX ?”\n",
    "\n",
    "➡️ Résultat : la requête est claire, et la recherche dans la base vectorielle peut retourner les bons documents.\n",
    "\n",
    "**👍 LangChain facilite ce processus**\n",
    "\n",
    "LangChain fournit une abstraction prête à l’emploi grâce à la classe `ConversationalRetrievalChain`.\n",
    "Cette classe prend automatiquement en charge :\n",
    "- la reformulation de la question via le LLM\n",
    "- la recherche dans la base vectorielle\n",
    "- la génération de la réponse finale à partir des documents récupérés et de l’historique\n",
    "\n",
    "➡️ Elle encapsule ainsi toute la logique conversationnelle d’un RAG en une seule ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80b0361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Vous :** quit"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de la conversation.\n"
     ]
    }
   ],
   "source": [
    "# Chaîne RAG avec historique\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=db.as_retriever())\n",
    "\n",
    "# Boucle de chat\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    clear_output(wait=True)                         # Efface l'affichage précédent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requête de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    result = qa_chain({\"question\": user_input, \"chat_history\": chat_history})\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "    chat_history.append((user_input, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a604a4",
   "metadata": {},
   "source": [
    "### 🧩 Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3873b2",
   "metadata": {},
   "source": [
    "Repartez de l'exercice précédent (NovTech), et implémentez un assistant de conversation continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8160aefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Vous :** quelles sont les procédures pour traiter les incidents du service billing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "According to the provided incident reports, the procedures for treating incidents on the service billing are:\n",
       "\n",
       "1. Redémarrage du service (Restarting the service)\n",
       "2. Ajout de monitoring spécifique (Adding specific monitoring)\n",
       "\n",
       "These two steps are mentioned as the corrective actions taken in all four incident reports (#30, #35, #37, and #45)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Vous :** stop"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de la conversation.\n"
     ]
    }
   ],
   "source": [
    "# Chaîne RAG avec historique\n",
    "qa_chain_novtech = ConversationalRetrievalChain.from_llm(llm=model, retriever=db.as_retriever())\n",
    "\n",
    "# Boucle de chat\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    #clear_output(wait=True)                         # Efface l'affichage précédent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requête de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    result = qa_chain_novtech.invoke({\"question\": user_input, \"chat_history\": chat_history})\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "    chat_history.append((user_input, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a8992",
   "metadata": {},
   "source": [
    "4. Graph DB (Neo4j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b52bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j\n",
      "  Downloading neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
      "Collecting pytz\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, neo4j\n",
      "Successfully installed neo4j-5.28.1 pytz-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aef085f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Triplets générés par le LLM :\n",
      " Voici les triplets extraites du texte :\n",
      "\n",
      "* (Albert Einstein)-[was]->(physicien théoricien)\n",
      "* (Albert Einstein)-[developed]->(théorie de la relativité)\n",
      "* (Albert Einstein)-[received]->(prix_Nobel_de_physique)\n",
      "* (Albert Einstein)-[received_in_year]->(1921)\n",
      "* (Albert Einstein)-[was_born]->(Allemagne)\n",
      "✅ Triplets insérés dans Neo4j.\n",
      "(* Albert Einstein) -[was]-> (physicien théoricien)\n",
      "(* Albert Einstein) -[developed]-> (théorie de la relativité)\n",
      "(* Albert Einstein) -[received]-> (prix_Nobel_de_physique)\n",
      "(* Albert Einstein) -[received_in_year]-> (1921)\n",
      "(* Albert Einstein) -[was_born]-> (Allemagne)\n",
      "\n",
      "📊 Résultat de la requête Cypher :\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import GraphQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 1. Connexion à Neo4j\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"test1234\"\n",
    ")\n",
    "\n",
    "# 2. Initialisation du LLM local via Ollama\n",
    "llm = ChatOllama(model=\"llama3\")  # ou mistral, gemma, etc.\n",
    "\n",
    "# 3. Texte brut à injecter\n",
    "text = \"\"\"\n",
    "Albert Einstein était un physicien théoricien. Il a développé la théorie de la relativité. \n",
    "Il a reçu le prix Nobel de physique en 1921. Il est né en Allemagne.\n",
    "\"\"\"\n",
    "\n",
    "# 4. Prompt d’extraction d'entités et de relations\n",
    "extraction_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Tu es un assistant qui extrait des faits sous forme de triplets (sujet, relation, objet).\n",
    "Relation doit être un seul mot ou plusieurs mots séparés par _.\n",
    "Voici un texte :\n",
    "\n",
    "{text}\n",
    "\n",
    "Donne-moi uniquement des triplets dans ce format :\n",
    "(Sujet)-[Relation]->(Objet)\n",
    "\n",
    "Relation doit être un seul mot ou plusieurs mots en anglais séparés par underscore (_).\n",
    "\n",
    "Inclue les relations suivantes si elles apparaissent :\n",
    "- received (pour les prix, récompenses)\n",
    "- received_in_year (pour l'année où un prix a été reçu)\n",
    "- was_born (lieu de naissance)\n",
    "\n",
    "Exemples :\n",
    "(Albert Einstein)-[developed]->(théorie de la relativité)\n",
    "(Albert Einstein)-[was_born]->(Allemagne)\n",
    "(Albert Einstein)-[received]->(prix_Nobel_de_physique)\n",
    "(Albert Einstein)-[received_in_year]->(1921)\n",
    "\"\"\")\n",
    "\n",
    "# 5. Génération des triplets à partir du texte\n",
    "triplet_prompt = extraction_prompt.format(text=text)\n",
    "response = llm.invoke(triplet_prompt)\n",
    "print(\"🔍 Triplets générés par le LLM :\\n\", response.content)\n",
    "\n",
    "# 6. Parser les triplets générés (très basique ici, à affiner)\n",
    "lines = response.content.strip().split(\"\\n\")\n",
    "triplets = []\n",
    "for line in lines:\n",
    "    if \"(\" in line and \")-[\" in line and \"]->(\" in line:\n",
    "        try:\n",
    "            subj = line.split(\")-[\")[0].replace(\"(\", \"\").strip()\n",
    "            rel = line.split(\")-[\")[1].split(\"]->\")[0].strip()\n",
    "            obj = line.split(\"]->(\")[1].replace(\")\", \"\").strip()\n",
    "            triplets.append((subj, rel, obj))\n",
    "        except Exception as e:\n",
    "            print(f\"⛔ Erreur de parsing : {e} sur la ligne : {line}\")\n",
    "\n",
    "# Suppression complète\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "# 7. Insertion dans Neo4j avec params sécurisés et relation nettoyée\n",
    "for subj, rel, obj in triplets:\n",
    "    clean_rel = rel.upper().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    cypher = (\n",
    "        \"MERGE (a:Entity {name: $subj}) \"\n",
    "        \"MERGE (b:Entity {name: $obj}) \"\n",
    "        f\"MERGE (a)-[:{clean_rel}]->(b)\"\n",
    "    )\n",
    "    graph.query(cypher, params={\"subj\": subj, \"obj\": obj})\n",
    "\n",
    "print(\"✅ Triplets insérés dans Neo4j.\")\n",
    "for subj, rel, obj in triplets:\n",
    "    print(f\"({subj}) -[{rel}]-> ({obj})\")\n",
    "\n",
    "# 8. Question sur le graphe : GraphQAChain NE SUPPORTE PAS Neo4jGraph directement.\n",
    "# On commente cette partie car ça provoque une erreur de type.\n",
    "# qa_chain = GraphQAChain.from_llm(llm=llm, graph=graph, verbose=True)\n",
    "# question = \"Quel prix Albert Einstein a-t-il reçu et en quelle année ?\"\n",
    "# answer = qa_chain.run(question)\n",
    "# print(\"\\n📢 Réponse :\", answer)\n",
    "\n",
    "# 9. Exemple d’interrogation Cypher directe avec Neo4j via graph.query()\n",
    "query = \"\"\"\n",
    "MATCH (a:Entity)-[r]->(b:Entity)\n",
    "WHERE a.name = \"Albert Einstein\" AND type(r) = \"RECEIVED\"\n",
    "RETURN b.name AS prix\n",
    "\"\"\"\n",
    "\n",
    "results = graph.query(query)\n",
    "print(\"\\n📊 Résultat de la requête Cypher :\")\n",
    "for record in results:\n",
    "    print(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b804dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-3.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "884ce3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graphe NetworkX créé avec les triplets.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from langchain_community.graphs import NetworkxEntityGraph\n",
    "\n",
    "\n",
    "# Exemple de triplets extraits (sujet, relation, objet)\n",
    "triplets = [\n",
    "    (\"Albert Einstein\", \"developed\", \"théorie de la relativité\"),\n",
    "    (\"Albert Einstein\", \"was_born\", \"Allemagne\"),\n",
    "    (\"Albert Einstein\", \"received\", \"prix Nobel de physique en 1921\"),\n",
    "]\n",
    "\n",
    "# Création d'un graphe orienté\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "\n",
    "\n",
    "# Ajout des triplets dans le graphe NetworkX\n",
    "for subj, rel, obj in triplets:\n",
    "    G.add_node(subj)\n",
    "    G.add_node(obj)\n",
    "    G.add_edge(subj, obj, label=rel)\n",
    "# G est ton MultiDiGraph avec les triplets\n",
    "nx_graph = NetworkxEntityGraph(G)\n",
    "print(\"✅ Graphe NetworkX créé avec les triplets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff5a62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10588\\3992415650.py:13: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa_chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphQAChain chain...\u001b[0m\n",
      "Entities Extracted:\n",
      "\u001b[32;1m\u001b[1;3mEinstein, Albert\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "📢 Réponse : I don't have this information, so I'll just say \"I don't know\".\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import GraphQAChain\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Initialiser le LLM (local via Ollama, ou autre)\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# Créer la chaîne GraphQAChain à partir du LLM et du graphe NetworkX\n",
    "qa_chain = GraphQAChain.from_llm(llm=llm, graph=nx_graph, verbose=True)\n",
    "\n",
    "\n",
    "# Poser une question\n",
    "question = \"Quel prix Albert Einstein a-t-il reçu et en quelle année ?\"\n",
    "answer = qa_chain.invoke(question)\n",
    "\n",
    "print(\"\\n📢 Réponse :\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
