{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "Les syst√®mes **RAG (Retrieval-Augmented Generation)** dans LangChain permettent aux mod√®les de langage de s‚Äôappuyer sur des **connaissances externes** pour produire des r√©ponses plus pr√©cises, actualis√©es et pertinentes.\n",
    "\n",
    "Contrairement √† un simple LLM qui g√©n√®re une r√©ponse uniquement √† partir de ce qu‚Äôil a appris pendant son entra√Ænement, un syst√®me RAG interroge une base de documents pour retrouver des morceaux d‚Äôinformation pertinents ‚Äì appel√©s **chunks** ‚Äì et les injecte dans le prompt du LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb160db",
   "metadata": {},
   "source": [
    "![RAG](img/rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3075ae",
   "metadata": {},
   "source": [
    "**Que montre le sch√©ma ci-dessus ?**\n",
    "\n",
    "Le processus se divise en **deux grandes phases** : **pr√©paration des documents** et **traitement des requ√™tes**.\n",
    "\n",
    "**Pr√©paration des documents (√† gauche)**\n",
    "- (1) Un fichier (document source) est divis√© en **chunks**, c‚Äôest-√†-dire en petits segments de texte.\n",
    "- (2) Chaque chunk est pass√© dans un LLM Embedder, un encodeur qui transforme le texte en un vecteur num√©rique (**embeddings**).\n",
    "- (3) Ces vecteurs sont ensuite stock√©s dans un Vector Store, une base de donn√©es sp√©cialis√©e pour les recherches par **similarit√© s√©mantique**.\n",
    "\n",
    "**Traitement des requ√™tes (√† droite)**\n",
    "- (a) Lorsqu‚Äôun utilisateur emet une requ√™te, celle-ci est √† son tour encod√©e via **le m√™me LLM Embedder** pour obtenir son vecteur.\n",
    "- (b) Ce vecteur est utilis√© par le **Retriever**, qui compare la requ√™te aux vecteurs des **chunks** pour trouver les plus similaires.\n",
    "- (c) Les chunks retrouv√©s sont envoy√©s au LLM, qui les utilise comme contexte pour formuler une r√©ponse.\n",
    "\n",
    "\n",
    "En r√©sum√©, ce fonctionnement est illustr√© par la boucle :\n",
    "\n",
    "> Requ√™te ‚Üí Encodage ‚Üí Recherche dans la base vectorielle ‚Üí R√©cup√©ration des chunks ‚Üí Passage au LLM ‚Üí R√©ponse contextuelle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du mod√®le LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d‚Äôinteragir facilement avec un mod√®le comme **llama3** ainsi qu'un **mod√®le d'embeddings** d√©j√† t√©l√©charg√©s via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7301c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Chargement des cl√©s d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3\", temperature=0)\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))\n",
    "\n",
    "# Mod√®le sp√©cialis√© pour convertir du texte en vecteurs (https://ollama.com/library/nomic-embed-text).\n",
    "# Il existe d'autres mod√®les d'embeddings (comme \"all-MiniLM-L6-v2\", \"text-embedding-ada-002\", etc.) \n",
    "# avec des performances et dimensions vari√©es selon les cas d‚Äôusage (recherche s√©mantique, classification, etc.).\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. RAG standard\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3369916",
   "metadata": {},
   "source": [
    "Le **RAG standard** consiste √† :\n",
    "- formuler une requ√™te explicite\n",
    "- interroger une base de documents vectoris√©e\n",
    "- utiliser un mod√®le LLM pour g√©n√©rer une r√©ponse √† partir des r√©sultats retrouv√©s. \n",
    " \n",
    "Ce pipeline est **efficace pour des questions ind√©pendantes, sans contexte conversationnel**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7de366",
   "metadata": {},
   "source": [
    "### 2.1 Pr√©paration des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699bd52",
   "metadata": {},
   "source": [
    "Nous initialisons les chemins n√©cessaires √† la pr√©paration des documents d‚Äôentr√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77475b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√®re le chemin absolu du r√©pertoire courant (l√† o√π le script est ex√©cut√©)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Nom du fichier texte contenant les comptes rendus de r√©union\n",
    "file_name = \"meeting_reports.txt\"\n",
    "\n",
    "# Construit le chemin complet vers le fichier texte dans le dossier \"data\"\n",
    "file_path = os.path.join(current_dir, \"data\", file_name)\n",
    "\n",
    "# D√©finit le chemin du r√©pertoire o√π sera stock√©e la base de donn√©es vectorielle (Chroma DB)\n",
    "db_dir = os.path.join(current_dir, \"data\", \"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed30a56",
   "metadata": {},
   "source": [
    "### 2.2 Initialisation du vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c526e6f",
   "metadata": {},
   "source": [
    "Nous v√©rifions ici si la base vectorielle existe d√©j√†.  \n",
    "Si ce n‚Äôest pas le cas, le fichier source est charg√©, d√©coup√© en morceaux, enrichi de m√©tadonn√©es, puis index√© dans Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9884d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(db_dir):\n",
    "    print(\"Initializing vector store...\")\n",
    "\n",
    "    # Chargement du fichier texte brut contenant les documents\n",
    "    loader = TextLoader(file_path)\n",
    "    loaded_document = loader.load()\n",
    "\n",
    "    # D√©coupage du document en chunks de 1000 caract√®res avec un chevauchement de 0\n",
    "    # - chunk_size d√©termine la taille maximale de chaque morceau (en nombre de caract√®res ici : 1000)\n",
    "    # - chunk_overlap permet de conserver un chevauchement entre les morceaux pour √©viter les coupures abruptes, ici il est √† 0, donc sans recouvrement.\n",
    "    # - RecursiveCharacterTextSplitter est souvent pr√©f√©r√© en pratique pour des documents textuels comme des comptes rendus, \n",
    "    #   des articles ou de la documentation technique, car il garde mieux le contexte s√©mantique.\n",
    "    #   Ce splitter tente d'abord de d√©couper sur les sauts de ligne, puis sur les phrases, puis sur les mots, etc.\n",
    "    # ... d'autres Text Splitter comme CharachterTextSplitter existent. √Ä approfondir si besoin\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "    # Ajout de m√©tadonn√©es √† chaque chunk (utile pour le filtrage ou le suivi de provenance).\n",
    "    # Ici 2 metadata sont ajout√©s mais il pourrait en y avoir plus.\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"source\"] = file_path    # Chemin d'origine du document\n",
    "        chunk.metadata[\"category\"] = \"meeting\"  # Cat√©gorie de contenu (√† adapter selon les besoins)\n",
    "\n",
    "    # Cr√©ation et persistance de la base vectorielle dans le dossier d√©fini\n",
    "    db = Chroma.from_documents(chunks, embedder, persist_directory=db_dir)\n",
    "\n",
    "    print(\"Vector store created !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbb986",
   "metadata": {},
   "source": [
    "### 2.3 Initialisation du moteur de recherche vectorielle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc3f2",
   "metadata": {},
   "source": [
    "Une fois la base vectorielle Chroma initialis√©e avec les embeddings, nous la transformons en **moteur de recherche (retriever)**.  \n",
    "Cela permet de retrouver les documents les plus proches s√©mantiquement d‚Äôune question ou d‚Äôune requ√™te.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee3ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8996\\573588974.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n"
     ]
    }
   ],
   "source": [
    "# Chargement de la base vectorielle existante, avec liaison avec le m√™me embedder ayant servi pour cr√©er la base vectorielle\n",
    "db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n",
    "\n",
    "# Conversion de la base Chroma en \"retriever\" pour effectuer des recherches par similarit√©\n",
    "# - search_type=\"similarity\" utilise la distance cosinus entre les vecteurs\n",
    "# - \"k\": 3 signifie que l'on souhaite r√©cup√©rer les 3 documents les plus proches\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# üí° Il est aussi possible d‚Äôutiliser d‚Äôautres types de recherche (search_type) :\n",
    "# - \"mmr\" (Maximal Marginal Relevance) : √©quilibre entre pertinence et diversit√© des r√©sultats\n",
    "# - \"similarity_score_threshold\" : retourne uniquement les documents dont le score d√©passe un certain seuil\n",
    "#      search_kwargs={\"score_threshold\": 0.8} permet par exemple de filtrer les r√©sultats peu pertinents\n",
    "#\n",
    "# D‚Äôautres param√®tres utiles dans search_kwargs :\n",
    "# - \"fetch_k\" : nombre de documents √† r√©cup√©rer avant le tri final (utile avec MMR)\n",
    "# - \"lambda_mult\" : pond√©ration entre pertinence et diversit√© dans MMR\n",
    "# \n",
    "# Etc... √† approfondir si besoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3318104",
   "metadata": {},
   "source": [
    "### 2.4 Ex√©cution d‚Äôune requ√™te de recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3186eb",
   "metadata": {},
   "source": [
    "Dans cette √©tape, nous combinons la recherche vectorielle avec un LLM.  \n",
    "L‚Äôobjectif est de fournir une r√©ponse pertinente √† une question, en s‚Äôappuyant uniquement sur les documents retrouv√©s dans la base vectorielle.  \n",
    "Le mod√®le est guid√© par un prompt structur√© qui inclut la requ√™te initiale et les contenus des chunks pertinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f69c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "En examinant les documents, j'ai trouv√© des informations relatives aux r√©unions de la soci√©t√© Neolink.\n",
       "\n",
       "Selon le document intitul√© \"Calendrier des r√©unions 2022\", il y a eu plusieurs r√©unions concernant la soci√©t√© Neolink au cours de l'ann√©e 2022. Voici quelques-unes de ces r√©unions :\n",
       "\n",
       "* R√©union du conseil d'administration : 15 mars 2022, 10h00\n",
       "* S√©ance de formation pour les nouveaux employ√©s : 22 mars 2022, 14h00\n",
       "* R√©union des √©quipes de vente : 5 avril 2022, 9h30\n",
       "* Assembl√©e g√©n√©rale annuelle : 12 mai 2022, 10h30\n",
       "* R√©union du comit√© de direction : 26 juillet 2022, 11h00\n",
       "* S√©ance de r√©troaction des employ√©s : 15 septembre 2022, 14h30\n",
       "\n",
       "Il est possible que d'autres r√©unions aient eu lieu ou soient pr√©vues, mais ces informations sont bas√©es sur les documents fournis.\n",
       "\n",
       "Si vous avez d'autres questions ou si vous souhaitez des informations suppl√©mentaires, n'h√©sitez pas √† me demander !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Requ√™te pos√©e par l'utilisateur\n",
    "query = \"Quels sont les r√©unions concernant la soci√©t√© Neolink ?\"\n",
    "\n",
    "# Recherche des chunks vectoriellement proches de la question\n",
    "relevant_chunks = retriever.invoke(query)\n",
    "\n",
    "# Optionnel : affichage manuel des chunks retrouv√©s (utile pour debug ou v√©rification)\n",
    "# for i, chunk in enumerate(relevant_chunks, 1):\n",
    "#     print(f\"Chunk {i}:\\n{chunk.page_content}\\n\")\n",
    "\n",
    "# Construction du message d'entr√©e √† envoyer au mod√®le\n",
    "# Nous incluons la question et le contenu des documents pour contraindre le LLM √† ne r√©pondre qu'en s'appuyant sur ces sources\n",
    "input_message = (\n",
    "    \"Voici des documents qui vont t'aider √† r√©pondre √† la question : \"\n",
    "    + query\n",
    "    + \"\\n\\nDocuments pertinents : \\n\"\n",
    "    + \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "    + \"\\n\\nDonne une r√©ponse bas√©e uniquement sur les documents qui te sont fournis.\"\n",
    ")\n",
    "\n",
    "# Construction du message complet pour le LLM, avec un r√¥le syst√®me et un message utilisateur\n",
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un assistant qui aide √† retrouver tout type d'informations interne √† une entreprise\"),\n",
    "    HumanMessage(content=input_message)\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c362eac",
   "metadata": {},
   "source": [
    "### üß© Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ccbc1",
   "metadata": {},
   "source": [
    "La soci√©t√© NovTech g√®re de nombreux documents internes :\n",
    "- des rapports d‚Äôincidents (panne, erreur technique, post-mortem),\n",
    "- des proc√©dures op√©rationnelles (onboarding, acc√®s syst√®me, d√©ploiement‚Ä¶).\n",
    "\n",
    "Actuellement, les √©quipes perdent du temps √† chercher les bonnes informations √† travers des fichiers √©parpill√©s.\n",
    "\n",
    "Votre objectif est de construire un assistant bas√© sur l'architecture RAG qui permettra :\n",
    "- de retrouver rapidement les proc√©dures en cas de besoin,\n",
    "- de consulter les r√©solutions d‚Äôincidents similaires,\n",
    "- de r√©pondre √† des questions en langage naturel en s‚Äôappuyant uniquement sur les documents internes.\n",
    "\n",
    "Pour vous aider, vous pouvez suivre les √©tapes suivantes :\n",
    "1. Chargement des documents\n",
    "2. D√©coupage en chunks\n",
    "3. Indexation vectorielle\n",
    "4. Recherche contextuelle\n",
    "5. G√©n√©ration de r√©ponse\n",
    "\n",
    "‚ÑπÔ∏è Les documents de l'entreprise se trouve dans le dossier `data/novtech`.  \n",
    "üí™üèª **Bonus** : Rendre possible un filtrage par cat√©gorie dans les recherches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25744402",
   "metadata": {},
   "source": [
    "#### 1. chargement des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffe3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def load_documents_from_folders(folders):\n",
    "    all_chunks = []\n",
    "    all_docs = []\n",
    "    print(\"forlders\",folders)\n",
    "    print(os.listdir(folders))\n",
    "    for folder in os.listdir(folders):\n",
    "        path_folder = os.path.join(folders,folder)\n",
    "        for file in os.listdir(path_folder):\n",
    "            print(file)\n",
    "            file_path = os.path.join(path_folder, file)\n",
    "            print(file_path)\n",
    "            if os.path.isfile(file_path):\n",
    "                loader = TextLoader(file_path)\n",
    "                loaded_document = loader.load()\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "                chunks = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "    # Ajout de m√©tadonn√©es √† chaque chunk (utile pour le filtrage ou le suivi de provenance).\n",
    "    # Ici 2 metadata sont ajout√©s mais il pourrait en y avoir plus.\n",
    "                for chunk in chunks:\n",
    "                    chunk.metadata[\"source\"] = file_path    # Chemin d'origine du document\n",
    "                    chunk.metadata[\"category\"] = folder.split(\"/\")[-1]  # Cat√©gorie de contenu (√† adapter selon les besoins)\n",
    "\n",
    "                all_chunks.extend(chunks)\n",
    "                all_docs.extend(loaded_document)\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103785c8",
   "metadata": {},
   "source": [
    "#### 2. Initialisation du db vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d078bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "    \n",
    "folders_path = os.path.join(current_dir,\"data/novtech/\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8bca326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forlders c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/\n",
      "['incidents', 'procedures']\n",
      "incident_01.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_01.txt\n",
      "incident_02.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_02.txt\n",
      "incident_03.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_03.txt\n",
      "incident_04.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_04.txt\n",
      "incident_05.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_05.txt\n",
      "incident_06.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_06.txt\n",
      "incident_07.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_07.txt\n",
      "incident_08.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_08.txt\n",
      "incident_09.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_09.txt\n",
      "incident_10.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_10.txt\n",
      "incident_11.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_11.txt\n",
      "incident_12.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_12.txt\n",
      "incident_13.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_13.txt\n",
      "incident_14.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_14.txt\n",
      "incident_15.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_15.txt\n",
      "incident_16.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_16.txt\n",
      "incident_17.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_17.txt\n",
      "incident_18.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_18.txt\n",
      "incident_19.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_19.txt\n",
      "incident_20.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_20.txt\n",
      "incident_21.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_21.txt\n",
      "incident_22.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_22.txt\n",
      "incident_23.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_23.txt\n",
      "incident_24.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_24.txt\n",
      "incident_25.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_25.txt\n",
      "incident_26.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_26.txt\n",
      "incident_27.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_27.txt\n",
      "incident_28.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_28.txt\n",
      "incident_29.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_29.txt\n",
      "incident_30.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_30.txt\n",
      "incident_31.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_31.txt\n",
      "incident_32.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_32.txt\n",
      "incident_33.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_33.txt\n",
      "incident_34.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_34.txt\n",
      "incident_35.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_35.txt\n",
      "incident_36.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_36.txt\n",
      "incident_37.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_37.txt\n",
      "incident_38.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_38.txt\n",
      "incident_39.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_39.txt\n",
      "incident_40.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_40.txt\n",
      "incident_41.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_41.txt\n",
      "incident_42.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_42.txt\n",
      "incident_43.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_43.txt\n",
      "incident_44.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_44.txt\n",
      "incident_45.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_45.txt\n",
      "incident_46.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_46.txt\n",
      "incident_47.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_47.txt\n",
      "incident_48.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_48.txt\n",
      "incident_49.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_49.txt\n",
      "incident_50.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/incidents\\incident_50.txt\n",
      "procedure_01.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_01.txt\n",
      "procedure_02.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_02.txt\n",
      "procedure_03.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_03.txt\n",
      "procedure_04.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_04.txt\n",
      "procedure_05.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_05.txt\n",
      "procedure_06.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_06.txt\n",
      "procedure_07.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_07.txt\n",
      "procedure_08.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_08.txt\n",
      "procedure_09.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_09.txt\n",
      "procedure_10.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_10.txt\n",
      "procedure_11.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_11.txt\n",
      "procedure_12.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_12.txt\n",
      "procedure_13.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_13.txt\n",
      "procedure_14.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_14.txt\n",
      "procedure_15.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_15.txt\n",
      "procedure_16.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_16.txt\n",
      "procedure_17.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_17.txt\n",
      "procedure_18.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_18.txt\n",
      "procedure_19.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_19.txt\n",
      "procedure_20.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_20.txt\n",
      "procedure_21.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_21.txt\n",
      "procedure_22.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_22.txt\n",
      "procedure_23.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_23.txt\n",
      "procedure_24.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_24.txt\n",
      "procedure_25.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_25.txt\n",
      "procedure_26.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_26.txt\n",
      "procedure_27.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_27.txt\n",
      "procedure_28.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_28.txt\n",
      "procedure_29.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_29.txt\n",
      "procedure_30.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_30.txt\n",
      "procedure_31.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_31.txt\n",
      "procedure_32.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_32.txt\n",
      "procedure_33.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_33.txt\n",
      "procedure_34.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_34.txt\n",
      "procedure_35.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_35.txt\n",
      "procedure_36.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_36.txt\n",
      "procedure_37.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_37.txt\n",
      "procedure_38.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_38.txt\n",
      "procedure_39.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_39.txt\n",
      "procedure_40.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_40.txt\n",
      "procedure_41.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_41.txt\n",
      "procedure_42.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_42.txt\n",
      "procedure_43.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_43.txt\n",
      "procedure_44.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_44.txt\n",
      "procedure_45.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_45.txt\n",
      "procedure_46.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_46.txt\n",
      "procedure_47.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_47.txt\n",
      "procedure_48.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_48.txt\n",
      "procedure_49.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_49.txt\n",
      "procedure_50.txt\n",
      "c:\\Users\\User\\Desktop\\LANGCHAIN_rag\\data/novtech/procedures\\procedure_50.txt\n"
     ]
    }
   ],
   "source": [
    "all_loaded_docs = load_documents_from_folders(folders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512928f",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " # D√©finit le chemin du r√©pertoire o√π sera stock√©e la base de donn√©es vectorielle (Chroma DB)\n",
    "db_dir_novtech = os.path.join(folders_path, \"db_novtech\")\n",
    "# Cr√©ation et persistance de la base vectorielle dans le dossier d√©fini\n",
    "db = Chroma.from_documents(all_loaded_docs, embedder, persist_directory=db_dir_novtech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "998f0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5536\\2472262417.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=db_dir_novtech, embedding_function=embedder)\n"
     ]
    }
   ],
   "source": [
    "# Chargement de la base vectorielle existante, avec liaison avec le m√™me embedder ayant servi pour cr√©er la base vectorielle\n",
    "db = Chroma(persist_directory=db_dir_novtech, embedding_function=embedder)\n",
    "\n",
    "# Conversion de la base Chroma en \"retriever\" pour effectuer des recherches par similarit√©\n",
    "# - search_type=\"similarity\" utilise la distance cosinus entre les vecteurs\n",
    "# - \"k\": 3 signifie que l'on souhaite r√©cup√©rer les 3 documents les plus proches\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3f439",
   "metadata": {},
   "source": [
    "#### 3 Ex√©cution d‚Äôune requ√™te de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49ae5cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Selon les rapports d'incident, il y a eu trois incidents concernant le service billing :\n",
       "\n",
       "* Incident #35 : erreur d√©tect√©e sur service billing, dur√©e 29 minutes, action corrective : red√©marrage du service et ajout de monitoring sp√©cifique.\n",
       "* Incident #30 : erreur d√©tect√©e sur service billing, dur√©e 39 minutes, action corrective : red√©marrage du service et ajout de monitoring sp√©cifique.\n",
       "* Incident #37 : erreur d√©tect√©e sur service billing, dur√©e 36 minutes, action corrective : red√©marrage du service et ajout de monitoring sp√©cifique.\n",
       "\n",
       "En r√©sum√©, il y a eu trois incidents concernant le service billing entre le 31 janvier 2024 et le 7 f√©vrier 2024."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Votre code ici\n",
    "\n",
    "# Requ√™te pos√©e par l'utilisateur\n",
    "query = \"Quels sont les incidents concernant le service billing ?\"\n",
    "\n",
    "# Recherche des chunks vectoriellement proches de la question\n",
    "relevant_chunks = retriever.invoke(query)\n",
    "\n",
    "# Optionnel : affichage manuel des chunks retrouv√©s (utile pour debug ou v√©rification)\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "     print(f\"Chunk {i}:\\n{chunk.page_content}\\n\")\n",
    "\n",
    "# Construction du message d'entr√©e √† envoyer au mod√®le\n",
    "# Nous incluons la question et le contenu des documents pour contraindre le LLM √† ne r√©pondre qu'en s'appuyant sur ces sources\n",
    "input_message = (\n",
    "    \"Voici des documents qui vont t'aider √† r√©pondre √† la question : \"\n",
    "    + query\n",
    "    + \"\\n\\nDocuments pertinents : \\n\"\n",
    "    + \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "    + \"\\n\\nDonne une r√©ponse bas√©e uniquement sur les documents qui te sont fournis.\"\n",
    ")\n",
    "\n",
    "# Construction du message complet pour le LLM, avec un r√¥le syst√®me et un message utilisateur\n",
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un assistant qui aide √† retrouver tout type d'informations interne √† une entreprise\"),\n",
    "    HumanMessage(content=input_message)\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a000884",
   "metadata": {},
   "source": [
    "# 3. RAG conversationnel\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843e3e1",
   "metadata": {},
   "source": [
    "Dans un cadre d‚Äô**interaction continue**, les utilisateurs posent souvent des questions implicites ou r√©f√©rentielles (ex. ‚ÄúEt lui ?‚Äù). Le **RAG conversationnel** ajoute une √©tape cl√© : la **reformulation de la question en prenant en compte l‚Äôhistorique du dialogue**.  \n",
    "\n",
    "Cette version de RAG permet de maintenir la pertinence des recherches dans la base vectorielle tout en conservant la fluidit√© de la conversation, ce qui la rend adapt√©e aux assistants IA ou aux chatbots avanc√©s.\n",
    "\n",
    "**Exemple**\n",
    "\n",
    "Historique de la conversation :\n",
    "- Utilisateur : *Qui est le CEO de Tesla ?*\n",
    "- IA : *Elon Musk est le CEO de Tesla*.\n",
    "- Utilisateur : *Et de SpaceX ?*\n",
    "\n",
    "‚û°Ô∏è La question ‚ÄúEt de SpaceX ?‚Äù est ambigu√´ seule. Le moteur de recherche (retriever) ne sait pas de quoi il s‚Äôagit exactement.\n",
    "\n",
    "Avec une reformulation de la question de l'utilisateur cela donnerait : ‚ÄúQui est le CEO de SpaceX ?‚Äù\n",
    "\n",
    "‚û°Ô∏è R√©sultat : la requ√™te est claire, et la recherche dans la base vectorielle peut retourner les bons documents.\n",
    "\n",
    "**üëç LangChain facilite ce processus**\n",
    "\n",
    "LangChain fournit une abstraction pr√™te √† l‚Äôemploi gr√¢ce √† la classe `ConversationalRetrievalChain`.\n",
    "Cette classe prend automatiquement en charge :\n",
    "- la reformulation de la question via le LLM\n",
    "- la recherche dans la base vectorielle\n",
    "- la g√©n√©ration de la r√©ponse finale √† partir des documents r√©cup√©r√©s et de l‚Äôhistorique\n",
    "\n",
    "‚û°Ô∏è Elle encapsule ainsi toute la logique conversationnelle d‚Äôun RAG en une seule ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80b0361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Vous :** quit"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de la conversation.\n"
     ]
    }
   ],
   "source": [
    "# Cha√Æne RAG avec historique\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=db.as_retriever())\n",
    "\n",
    "# Boucle de chat\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    clear_output(wait=True)                         # Efface l'affichage pr√©c√©dent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requ√™te de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    result = qa_chain({\"question\": user_input, \"chat_history\": chat_history})\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "    chat_history.append((user_input, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a604a4",
   "metadata": {},
   "source": [
    "### üß© Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3873b2",
   "metadata": {},
   "source": [
    "Repartez de l'exercice pr√©c√©dent (NovTech), et impl√©mentez un assistant de conversation continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8160aefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Vous :** quelles sont les proc√©dures pour traiter les incidents du service billing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "According to the provided incident reports, the procedures for treating incidents on the service billing are:\n",
       "\n",
       "1. Red√©marrage du service (Restarting the service)\n",
       "2. Ajout de monitoring sp√©cifique (Adding specific monitoring)\n",
       "\n",
       "These two steps are mentioned as the corrective actions taken in all four incident reports (#30, #35, #37, and #45)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Vous :** stop"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de la conversation.\n"
     ]
    }
   ],
   "source": [
    "# Cha√Æne RAG avec historique\n",
    "qa_chain_novtech = ConversationalRetrievalChain.from_llm(llm=model, retriever=db.as_retriever())\n",
    "\n",
    "# Boucle de chat\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    #clear_output(wait=True)                         # Efface l'affichage pr√©c√©dent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requ√™te de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    result = qa_chain_novtech.invoke({\"question\": user_input, \"chat_history\": chat_history})\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "    chat_history.append((user_input, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a8992",
   "metadata": {},
   "source": [
    "4. Graph DB (Neo4j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b52bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j\n",
      "  Downloading neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
      "Collecting pytz\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, neo4j\n",
      "Successfully installed neo4j-5.28.1 pytz-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aef085f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Triplets g√©n√©r√©s par le LLM :\n",
      " Voici les triplets extraites du texte :\n",
      "\n",
      "* (Albert Einstein)-[was]->(physicien th√©oricien)\n",
      "* (Albert Einstein)-[developed]->(th√©orie de la relativit√©)\n",
      "* (Albert Einstein)-[received]->(prix_Nobel_de_physique)\n",
      "* (Albert Einstein)-[received_in_year]->(1921)\n",
      "* (Albert Einstein)-[was_born]->(Allemagne)\n",
      "‚úÖ Triplets ins√©r√©s dans Neo4j.\n",
      "(* Albert Einstein) -[was]-> (physicien th√©oricien)\n",
      "(* Albert Einstein) -[developed]-> (th√©orie de la relativit√©)\n",
      "(* Albert Einstein) -[received]-> (prix_Nobel_de_physique)\n",
      "(* Albert Einstein) -[received_in_year]-> (1921)\n",
      "(* Albert Einstein) -[was_born]-> (Allemagne)\n",
      "\n",
      "üìä R√©sultat de la requ√™te Cypher :\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import GraphQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 1. Connexion √† Neo4j\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"test1234\"\n",
    ")\n",
    "\n",
    "# 2. Initialisation du LLM local via Ollama\n",
    "llm = ChatOllama(model=\"llama3\")  # ou mistral, gemma, etc.\n",
    "\n",
    "# 3. Texte brut √† injecter\n",
    "text = \"\"\"\n",
    "Albert Einstein √©tait un physicien th√©oricien. Il a d√©velopp√© la th√©orie de la relativit√©. \n",
    "Il a re√ßu le prix Nobel de physique en 1921. Il est n√© en Allemagne.\n",
    "\"\"\"\n",
    "\n",
    "# 4. Prompt d‚Äôextraction d'entit√©s et de relations\n",
    "extraction_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Tu es un assistant qui extrait des faits sous forme de triplets (sujet, relation, objet).\n",
    "Relation doit √™tre un seul mot ou plusieurs mots s√©par√©s par _.\n",
    "Voici un texte :\n",
    "\n",
    "{text}\n",
    "\n",
    "Donne-moi uniquement des triplets dans ce format :\n",
    "(Sujet)-[Relation]->(Objet)\n",
    "\n",
    "Relation doit √™tre un seul mot ou plusieurs mots en anglais s√©par√©s par underscore (_).\n",
    "\n",
    "Inclue les relations suivantes si elles apparaissent :\n",
    "- received (pour les prix, r√©compenses)\n",
    "- received_in_year (pour l'ann√©e o√π un prix a √©t√© re√ßu)\n",
    "- was_born (lieu de naissance)\n",
    "\n",
    "Exemples :\n",
    "(Albert Einstein)-[developed]->(th√©orie de la relativit√©)\n",
    "(Albert Einstein)-[was_born]->(Allemagne)\n",
    "(Albert Einstein)-[received]->(prix_Nobel_de_physique)\n",
    "(Albert Einstein)-[received_in_year]->(1921)\n",
    "\"\"\")\n",
    "\n",
    "# 5. G√©n√©ration des triplets √† partir du texte\n",
    "triplet_prompt = extraction_prompt.format(text=text)\n",
    "response = llm.invoke(triplet_prompt)\n",
    "print(\"üîç Triplets g√©n√©r√©s par le LLM :\\n\", response.content)\n",
    "\n",
    "# 6. Parser les triplets g√©n√©r√©s (tr√®s basique ici, √† affiner)\n",
    "lines = response.content.strip().split(\"\\n\")\n",
    "triplets = []\n",
    "for line in lines:\n",
    "    if \"(\" in line and \")-[\" in line and \"]->(\" in line:\n",
    "        try:\n",
    "            subj = line.split(\")-[\")[0].replace(\"(\", \"\").strip()\n",
    "            rel = line.split(\")-[\")[1].split(\"]->\")[0].strip()\n",
    "            obj = line.split(\"]->(\")[1].replace(\")\", \"\").strip()\n",
    "            triplets.append((subj, rel, obj))\n",
    "        except Exception as e:\n",
    "            print(f\"‚õî Erreur de parsing : {e} sur la ligne : {line}\")\n",
    "\n",
    "# Suppression compl√®te\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "# 7. Insertion dans Neo4j avec params s√©curis√©s et relation nettoy√©e\n",
    "for subj, rel, obj in triplets:\n",
    "    clean_rel = rel.upper().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    cypher = (\n",
    "        \"MERGE (a:Entity {name: $subj}) \"\n",
    "        \"MERGE (b:Entity {name: $obj}) \"\n",
    "        f\"MERGE (a)-[:{clean_rel}]->(b)\"\n",
    "    )\n",
    "    graph.query(cypher, params={\"subj\": subj, \"obj\": obj})\n",
    "\n",
    "print(\"‚úÖ Triplets ins√©r√©s dans Neo4j.\")\n",
    "for subj, rel, obj in triplets:\n",
    "    print(f\"({subj}) -[{rel}]-> ({obj})\")\n",
    "\n",
    "# 8. Question sur le graphe : GraphQAChain NE SUPPORTE PAS Neo4jGraph directement.\n",
    "# On commente cette partie car √ßa provoque une erreur de type.\n",
    "# qa_chain = GraphQAChain.from_llm(llm=llm, graph=graph, verbose=True)\n",
    "# question = \"Quel prix Albert Einstein a-t-il re√ßu et en quelle ann√©e ?\"\n",
    "# answer = qa_chain.run(question)\n",
    "# print(\"\\nüì¢ R√©ponse :\", answer)\n",
    "\n",
    "# 9. Exemple d‚Äôinterrogation Cypher directe avec Neo4j via graph.query()\n",
    "query = \"\"\"\n",
    "MATCH (a:Entity)-[r]->(b:Entity)\n",
    "WHERE a.name = \"Albert Einstein\" AND type(r) = \"RECEIVED\"\n",
    "RETURN b.name AS prix\n",
    "\"\"\"\n",
    "\n",
    "results = graph.query(query)\n",
    "print(\"\\nüìä R√©sultat de la requ√™te Cypher :\")\n",
    "for record in results:\n",
    "    print(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b804dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-3.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "884ce3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graphe NetworkX cr√©√© avec les triplets.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from langchain_community.graphs import NetworkxEntityGraph\n",
    "\n",
    "\n",
    "# Exemple de triplets extraits (sujet, relation, objet)\n",
    "triplets = [\n",
    "    (\"Albert Einstein\", \"developed\", \"th√©orie de la relativit√©\"),\n",
    "    (\"Albert Einstein\", \"was_born\", \"Allemagne\"),\n",
    "    (\"Albert Einstein\", \"received\", \"prix Nobel de physique en 1921\"),\n",
    "]\n",
    "\n",
    "# Cr√©ation d'un graphe orient√©\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "\n",
    "\n",
    "# Ajout des triplets dans le graphe NetworkX\n",
    "for subj, rel, obj in triplets:\n",
    "    G.add_node(subj)\n",
    "    G.add_node(obj)\n",
    "    G.add_edge(subj, obj, label=rel)\n",
    "# G est ton MultiDiGraph avec les triplets\n",
    "nx_graph = NetworkxEntityGraph(G)\n",
    "print(\"‚úÖ Graphe NetworkX cr√©√© avec les triplets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff5a62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10588\\3992415650.py:13: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa_chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphQAChain chain...\u001b[0m\n",
      "Entities Extracted:\n",
      "\u001b[32;1m\u001b[1;3mEinstein, Albert\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "üì¢ R√©ponse : I don't have this information, so I'll just say \"I don't know\".\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import GraphQAChain\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Initialiser le LLM (local via Ollama, ou autre)\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# Cr√©er la cha√Æne GraphQAChain √† partir du LLM et du graphe NetworkX\n",
    "qa_chain = GraphQAChain.from_llm(llm=llm, graph=nx_graph, verbose=True)\n",
    "\n",
    "\n",
    "# Poser une question\n",
    "question = \"Quel prix Albert Einstein a-t-il re√ßu et en quelle ann√©e ?\"\n",
    "answer = qa_chain.invoke(question)\n",
    "\n",
    "print(\"\\nüì¢ R√©ponse :\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
